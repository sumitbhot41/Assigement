{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c8b46-9b3b-4c57-a563-d20d7d1c211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "Q4. How do you measure the performance of KNN?\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "Q6. How do you handle missing values in KNN?\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cd122c-059f-48c3-9cfa-6ef5f8dfb509",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q1. **What is the KNN algorithm?**\n",
    "   KNN stands for K-Nearest Neighbors. It's a simple and widely used algorithm for classification and regression tasks in machine learning. In KNN, the output (class or value) of a data point is determined by the majority class or average value of its k nearest neighbors in the feature space. It's a non-parametric and lazy learning algorithm because it doesn't make assumptions about the underlying data distribution and doesn't learn a specific model during training.\n",
    "\n",
    "Q2. **How do you choose the value of K in KNN?**\n",
    "   The choice of the value of K significantly affects the performance of the KNN algorithm. A smaller value of K can lead to more flexible decision boundaries but may also make the model sensitive to noise. Conversely, a larger value of K can provide smoother decision boundaries but may lead to over-smoothing and reduced model flexibility. The value of K is typically chosen through hyperparameter tuning, using techniques such as cross-validation.\n",
    "\n",
    "Q3. **What is the difference between KNN classifier and KNN regressor?**\n",
    "   - KNN Classifier: In KNN classification, the output is a class label assigned to the majority class among the k nearest neighbors of a data point.\n",
    "   - KNN Regressor: In KNN regression, the output is a continuous value computed as the average (or another aggregation) of the target values of the k nearest neighbors.\n",
    "\n",
    "Q4. **How do you measure the performance of KNN?**\n",
    "   The performance of KNN can be measured using various evaluation metrics depending on the task, such as accuracy, precision, recall, F1-score for classification, and mean squared error, R-squared for regression. Cross-validation can also be used to get a more robust estimate of performance.\n",
    "\n",
    "Q5. **What is the curse of dimensionality in KNN?**\n",
    "   The curse of dimensionality refers to the phenomenon where the performance of KNN deteriorates as the number of features (dimensions) increases. In high-dimensional spaces, data points become sparse, making it difficult to find meaningful nearest neighbors. This can lead to increased computational complexity and reduced predictive performance.\n",
    "\n",
    "Q6. **How do you handle missing values in KNN?**\n",
    "   One common approach to handling missing values in KNN is to impute them by computing the average or median of the available values for the missing feature across the dataset. Another approach is to use distance-weighted imputation, where missing values are estimated based on the values of the nearest neighbors.\n",
    "\n",
    "Q7. **Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**\n",
    "   - KNN Classifier: Suitable for classification problems where the decision boundary is non-linear and the class distribution is not well-separated.\n",
    "   - KNN Regressor: Suitable for regression problems where the target variable shows local dependencies and smooth variations across the feature space.\n",
    "   The choice between classifier and regressor depends on the nature of the problem and the underlying data distribution.\n",
    "\n",
    "Q8. **What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**\n",
    "   Strengths:\n",
    "   - Simple and easy to understand.\n",
    "   - Non-parametric and does not make strong assumptions about the underlying data distribution.\n",
    "   - Effective for both classification and regression tasks, especially for small to medium-sized datasets.\n",
    "   Weaknesses:\n",
    "   - Computationally expensive, especially for large datasets or high-dimensional feature spaces.\n",
    "   - Sensitive to irrelevant features and noisy data.\n",
    "   - Requires careful preprocessing and feature scaling.\n",
    "   These weaknesses can be addressed through dimensionality reduction techniques, feature selection, and careful hyperparameter tuning.\n",
    "\n",
    "Q9. **What is the difference between Euclidean distance and Manhattan distance in KNN?**\n",
    "   - Euclidean distance: It measures the straight-line distance between two points in the feature space, calculated as the square root of the sum of squared differences of corresponding coordinates.\n",
    "   - Manhattan distance: Also known as city block distance, it measures the distance between two points as the sum of absolute differences of their coordinates along each dimension.\n",
    "   The choice between Euclidean and Manhattan distance depends on the nature of the data and the problem at hand.\n",
    "\n",
    "Q10. **What is the role of feature scaling in KNN?**\n",
    "    Feature scaling is crucial in KNN because it ensures that all features contribute equally to the distance computation between data points. Without feature scaling, features with larger scales may dominate the distance calculation, leading to biased results. Common techniques for feature scaling include standardization (scaling to have mean 0 and standard deviation 1) and normalization (scaling to a range, e.g., [0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0df33c-cd93-40af-a907-d9acef8f07cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
