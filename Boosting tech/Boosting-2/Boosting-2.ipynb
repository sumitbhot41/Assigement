{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6ad09-1470-4d82-a421-16f8745b7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Q1. **What is Gradient Boosting Regression?**\n",
    "   Gradient Boosting Regression is a machine learning technique that builds an ensemble of weak regression models, typically decision trees, by sequentially fitting new models to the residual errors of the previous models. It optimizes a loss function, usually the mean squared error, by minimizing the gradient of the loss with respect to the predicted values.\n",
    "\n",
    "\n",
    "Q3. **Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model. Use grid search or random search to find the best hyperparameters.**\n",
    "   You can use libraries like `GridSearchCV` from scikit-learn for hyperparameter tuning.\n",
    "\n",
    "Q4. **What is a weak learner in Gradient Boosting?**\n",
    "   A weak learner in Gradient Boosting is a model that performs slightly better than random guessing on a given problem. Typically, decision trees with shallow depths (e.g., depth 1 or 2) are used as weak learners.\n",
    "\n",
    "Q5. **What is the intuition behind the Gradient Boosting algorithm?**\n",
    "   The intuition behind Gradient Boosting is to iteratively improve the predictions by focusing on the errors made by previous models. Each new model in the ensemble learns to correct the errors of the previous models, gradually reducing the overall prediction error.\n",
    "\n",
    "Q6. **How does Gradient Boosting algorithm build an ensemble of weak learners?**\n",
    "   Gradient Boosting builds an ensemble of weak learners by sequentially fitting new models to the residual errors of the previous models. Each new model focuses on the errors made by the ensemble so far, gradually improving the overall prediction.\n",
    "\n",
    "Q7. **What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?**\n",
    "   The steps involved in constructing the mathematical intuition of Gradient Boosting include:\n",
    "   - Defining a loss function to optimize (e.g., mean squared error).\n",
    "   - Fitting an initial model to predict the average target value.\n",
    "   - Calculating the gradient of the loss function with respect to the predictions.\n",
    "   - Training subsequent models to predict the negative gradient (residuals) of the loss function.\n",
    "   - Combining predictions of all models in the ensemble using a weighted sum to minimize the overall loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0313dc3-2ad1-411c-af94-bbb4e11d6d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 31.4050521317727\n",
      "R-squared: 0.9775058843017153\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full_like(y, np.mean(y))  # Initial prediction: mean of y\n",
    "        for _ in range(self.n_estimators):\n",
    "            residual = y - y_pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            self.models.append(tree)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d341f56-46c5-4e93-b4e0-5f6fd59f18fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
